{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stemming, Lemmatization and Stop words.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMBfpXcMD2chwO3xaxzaW8X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kunal24bit/NLP/blob/main/Stemming%2C_Lemmatization_and_Stop_words.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYxuwqYEOKmM"
      },
      "source": [
        "**Stemming**\r\n",
        "\r\n",
        "Often when searching for a *certain* keyword, it helps if the search returns variations of the word. For instance searching for look might return looks or looking. Here look would be the stem for looks, looking.\r\n",
        "\r\n",
        "Stemming is somewhat a crude method for cataloging related words. It essentially chops off letter from the end until the stem is reached.\r\n",
        "\r\n",
        "It might be surprising to you but spaCy doesn't contain any function for stemming as it relies on lemmatization only. Therefore, in this section, we will use NLTK for stemming.\r\n",
        "\r\n",
        "NLTK stands for Natural Language tool kit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1Xk6FctPJxH"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1C2Vlf0QG-f"
      },
      "source": [
        "from nltk.stem.porter import PorterStemmer #Porter stemmer is a Stemming algorithm developed by martin Porter in 1980. It used some rules "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8M5Slu3IQRiX"
      },
      "source": [
        "p_stemmer = PorterStemmer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYpOjxQQQa5-"
      },
      "source": [
        "words = [\"run\", \"runner\", \"ran\", \"runs\", \"easily\", \"fairly\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnr0DRjOQoY-",
        "outputId": "a22fdb7a-24a1-49b2-def6-e4c704f9680f"
      },
      "source": [
        "for word in words:\r\n",
        "  print(word + \"----->\" + p_stemmer.stem(word))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "run----->run\n",
            "runner----->runner\n",
            "ran----->ran\n",
            "runs----->run\n",
            "easily----->easili\n",
            "fairly----->fairli\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UtSYS2RRRNn"
      },
      "source": [
        "from nltk.stem.snowball import SnowballStemmer # SNow ball is also a Stemming language developed by Martin Porter which is faster and more effircient than Porter stemmer."
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhJ5wDJjRhr3"
      },
      "source": [
        "s_stemmer = SnowballStemmer(language=\"english\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SZGwS73RmcO"
      },
      "source": [
        "words = [\"run\", \"runner\", \"ran\", \"runs\", \"easily\", \"fairly\"]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcoP0wvkR17w",
        "outputId": "3a3327e7-147c-45e7-83b8-38d99af9538a"
      },
      "source": [
        "for word in words:\r\n",
        "  print(word + \"----->\" + s_stemmer.stem(word))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "run----->run\n",
            "runner----->runner\n",
            "ran----->ran\n",
            "runs----->run\n",
            "easily----->easili\n",
            "fairly----->fair\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z49RAnBxR-UX"
      },
      "source": [
        "#One more example\r\n",
        "\r\n",
        "words = [\"generous\", \"generation\", \"generously\", \"generate\"]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WYSB2ib1xPc",
        "outputId": "b663a2b9-4776-474b-f698-e773a3dbc017"
      },
      "source": [
        "for word in words:\r\n",
        "  print(word + \"----->\" + s_stemmer.stem(word))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "generous----->generous\n",
            "generation----->generat\n",
            "generously----->generous\n",
            "generate----->generat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rinvwuyF2Uww"
      },
      "source": [
        "**Lemmatization**\r\n",
        "\r\n",
        "Lemmatization looks beyond word reduction and considers a language full vocabulary to apply a morphological analysis to words.\r\n",
        "\r\n",
        "The Lemma of \"was\" is \"be\" and lemma of \"mice\" is \"mouse\". Futher the lemma of \"meeting\" might be \"meet\" or \"meeting\" depending on its use in sentence.\r\n",
        "\r\n",
        "Lemmatization is seen as much more informative than simple stemming. That is why SpaCy has opted only to have lemmatization instead of stemming.\r\n",
        "\r\n",
        "Lemmatization looks at sorrounding text to determine a guven parts of speech, It does not categorize phrases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Is-k4yX-3F9E"
      },
      "source": [
        "import spacy"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmlfGAvf39w1"
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSLPfwSK4Cqk"
      },
      "source": [
        "doc1 = nlp(u\"I am runner running in a race because I love to run since I ran today.\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wt5cIzfF5QWM",
        "outputId": "50af7174-8ea5-463b-c442-3df306d41d8a"
      },
      "source": [
        "for token in doc1:\r\n",
        "  print(token.text,'\\t', token.pos_, '\\t', token.lemma, '\\t', token.lemma_ )"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I \t PRON \t 561228191312463089 \t -PRON-\n",
            "am \t AUX \t 10382539506755952630 \t be\n",
            "runner \t PROPN \t 12640964157389618806 \t runner\n",
            "running \t VERB \t 12767647472892411841 \t run\n",
            "in \t ADP \t 3002984154512732771 \t in\n",
            "a \t DET \t 11901859001352538922 \t a\n",
            "race \t NOUN \t 8048469955494714898 \t race\n",
            "because \t SCONJ \t 16950148841647037698 \t because\n",
            "I \t PRON \t 561228191312463089 \t -PRON-\n",
            "love \t VERB \t 3702023516439754181 \t love\n",
            "to \t PART \t 3791531372978436496 \t to\n",
            "run \t VERB \t 12767647472892411841 \t run\n",
            "since \t SCONJ \t 10066841407251338481 \t since\n",
            "I \t PRON \t 561228191312463089 \t -PRON-\n",
            "ran \t VERB \t 12767647472892411841 \t run\n",
            "today \t NOUN \t 11042482332948150395 \t today\n",
            ". \t PUNCT \t 12646065887601541794 \t .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvvV-4Ty6ccq"
      },
      "source": [
        "Running, run and ran reduced to same lemma and they have same hash value 12767647472892411841."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VH2hNg075x0j"
      },
      "source": [
        "def show_lemma(text):\r\n",
        "  for token in text:\r\n",
        "    print(f'{token.text:{12}},{token.pos_:{6}},{token.lemma:<{22}},{token.lemma_:{10}} ')"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92ihmheQ8yze",
        "outputId": "e2cec4b3-4cf2-4f25-cc7a-2addcb6e1356"
      },
      "source": [
        "\r\n",
        "show_lemma(doc1) #Now its printing in a good format."
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I           ,PRON  ,561228191312463089    ,-PRON-     \n",
            "am          ,AUX   ,10382539506755952630  ,be         \n",
            "runner      ,PROPN ,12640964157389618806  ,runner     \n",
            "running     ,VERB  ,12767647472892411841  ,run        \n",
            "in          ,ADP   ,3002984154512732771   ,in         \n",
            "a           ,DET   ,11901859001352538922  ,a          \n",
            "race        ,NOUN  ,8048469955494714898   ,race       \n",
            "because     ,SCONJ ,16950148841647037698  ,because    \n",
            "I           ,PRON  ,561228191312463089    ,-PRON-     \n",
            "love        ,VERB  ,3702023516439754181   ,love       \n",
            "to          ,PART  ,3791531372978436496   ,to         \n",
            "run         ,VERB  ,12767647472892411841  ,run        \n",
            "since       ,SCONJ ,10066841407251338481  ,since      \n",
            "I           ,PRON  ,561228191312463089    ,-PRON-     \n",
            "ran         ,VERB  ,12767647472892411841  ,run        \n",
            "today       ,NOUN  ,11042482332948150395  ,today      \n",
            ".           ,PUNCT ,12646065887601541794  ,.          \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xu39eEGH-O0D"
      },
      "source": [
        "**Stop Words**\r\n",
        "\r\n",
        "Words like \"a\" and \"the\" appears so frequently that they do not require tagging like noun and modifiers. We call these words Stop words and they can be filtered from the text to be processed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Lui0RKe-Qqk",
        "outputId": "9c07fe2c-6aa9-45d5-deb2-4ec12a5721d8"
      },
      "source": [
        "len(nlp.Defaults.stop_words)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "326"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYW-tuHH-2-W"
      },
      "source": [
        "#Adding a stop word in library\r\n",
        "\r\n",
        "nlp.Defaults.stop_words.add('btw')"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3SKix4A_JOt",
        "outputId": "81311fe4-c63a-4c1e-ff98-6a1a1d56cd13"
      },
      "source": [
        "print(nlp.Defaults.stop_words)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'whence', 'wherever', 'latterly', 'n’t', 'please', 'myself', 'herself', 'none', 'eight', '’ve', 'thereafter', 'than', 'yours', 'around', 'perhaps', 'noone', 'them', 'whereby', 'there', 'where', 'nor', 'five', 'such', 'btw', 'using', 'whether', 'own', 'hence', 'could', 'off', '’d', 'make', 'however', 'that', 'former', 'empty', 'on', 'others', '’re', 'him', 'anything', 'herein', 'yourself', 'already', 'as', 'ca', 'per', 'ever', 'call', 'due', 'yet', 'twelve', 'here', 'throughout', 'fifty', 'is', 'also', 'will', 'into', 'under', 'after', 'bottom', 'then', 'while', 'he', 'against', 'anywhere', 'used', 'been', 'thereby', 'this', 'even', 'himself', 'though', 'put', 'really', 'whole', 'so', 'does', 'his', 'out', 'nobody', 'became', 'doing', 'what', 'should', 'would', 'from', 'whither', 'mine', 'hereafter', '’m', 'being', 'often', 'still', 'down', 'enough', 'themselves', 'few', 'whereafter', 'may', 'seemed', 'mostly', 'were', 'other', 'why', 'twenty', 'everyone', 'those', 'becomes', 'any', 're', 'keep', 'do', 'hereupon', 'nevertheless', 'whereas', 'regarding', 'for', 'whoever', '‘ll', 'each', 'we', 'both', 'except', 'these', \"n't\", 'can', 'say', 'never', 'one', 'side', 'at', 'our', 'next', 'she', 'thence', 'seem', 'you', '‘ve', 'serious', 'ten', 'until', 'some', 'another', 'not', 'too', 'thereupon', 'along', 'together', 'seems', 'who', 'must', 'somehow', 'alone', 'over', 'of', 'whereupon', 'hers', 'above', 'someone', 'yourselves', 'in', 'top', 'toward', 'somewhere', 'further', 'move', 'two', 'therefore', 'amongst', 'become', 'take', 'formerly', 'by', 'among', 'before', 'else', 'whatever', 'most', 'ours', 'name', 'across', 'wherein', 'front', '‘m', 'i', 'becoming', 'fifteen', 'within', 'which', 'always', 'again', 'back', 'her', 'anyone', 'neither', 'are', 'with', 'otherwise', 'via', '‘re', 'thus', 'towards', 'quite', 'might', 'moreover', 'sometimes', 'three', 'namely', 'now', 'your', 'everywhere', 'last', 'they', '’s', 'either', 'the', 'because', 'elsewhere', 'during', 'something', 'or', 'unless', 'but', 'amount', 'hereby', 'rather', 'no', 'was', 'sometime', 'once', 'about', 'afterwards', 'just', 'first', 'several', 'its', 'indeed', 'six', 'am', 'anyhow', 'thru', 'third', 'sixty', 'their', 'nothing', 'how', 'get', 'therein', 'forty', 'done', 'whose', 'besides', 'four', 'less', \"'s\", 'go', 'have', 'an', 'many', 'much', 'to', \"'ll\", 'seeming', 'full', 'beyond', 'below', 'eleven', 'beforehand', 'between', 'had', 'everything', 'although', 'whenever', '‘s', 'me', 'beside', 'more', 'itself', 'upon', \"'ve\", 'onto', 'whom', 'latter', 'since', 'without', 'if', 'well', 'least', 'up', 'almost', 'nine', 'did', 'cannot', 'give', 'meanwhile', \"'m\", \"'re\", '’ll', 'part', 'nowhere', 'show', 'ourselves', 'through', 'my', 'every', 'only', 'behind', 'same', 'be', 'see', 'n‘t', 'and', 'anyway', 'all', 'it', 'made', 'very', 'hundred', 'various', 'has', 'us', 'when', \"'d\", '‘d', 'a'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAoTtR66_Rct",
        "outputId": "3109bf01-758c-4e33-b9ae-16afe7fe846e"
      },
      "source": [
        "len(nlp.Defaults.stop_words)#btw  is added."
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "327"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KD6B37QL_WGM",
        "outputId": "a3ed9035-7e33-434b-b7c9-042491f332ba"
      },
      "source": [
        "nlp.vocab['btw'].is_stop"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzcF66CA_xZl"
      },
      "source": [
        "#Removing a stop word\r\n",
        "\r\n",
        "nlp.Defaults.stop_words.remove(\"beyond\")"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtvtWP8XAKxd",
        "outputId": "a549ed46-a125-451c-a4aa-ca11f82d6275"
      },
      "source": [
        "nlp.vocab['beyond'].is_stop"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10cszPTcARQd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}